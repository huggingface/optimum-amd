--- /home/ubuntu/miniconda2/envs/optimummain/lib/python3.8/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py	2023-11-06 13:18:44.884610884 +0000
+++ modeling_gpt_bigcode.py	2023-11-03 11:48:08.528225345 +0000
@@ -21,22 +21,22 @@
 from torch import nn
 from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
 
-from ...activations import ACT2FN
-from ...modeling_outputs import (
+from transformers.activations import ACT2FN
+from transformers.modeling_outputs import (
     BaseModelOutputWithPastAndCrossAttentions,
     CausalLMOutputWithCrossAttentions,
     SequenceClassifierOutputWithPast,
     TokenClassifierOutput,
 )
-from ...modeling_utils import PreTrainedModel
-from ...utils import (
+from transformers.modeling_utils import PreTrainedModel
+from transformers.utils import (
     add_code_sample_docstrings,
     add_start_docstrings,
     add_start_docstrings_to_model_forward,
     is_flash_attn_2_available,
     logging,
 )
-from .configuration_gpt_bigcode import GPTBigCodeConfig
+from transformers.models.gpt_bigcode.configuration_gpt_bigcode import GPTBigCodeConfig
 
 
 if is_flash_attn_2_available():
@@ -269,7 +269,7 @@
             )
 
         if layer_past is not None:
-            key_value = torch.cat((layer_past, key_value), dim=-2)
+            key_value = torch.cat((layer_past, key_value), dim=-2)[:, 1:, :]
         present = key_value if use_cache else None
 
         key, value = key_value.split((self.head_dim, self.head_dim), dim=-1)
@@ -847,7 +847,8 @@
         # Self-attention mask.
         query_length = input_shape[-1]
         key_length = past_length + query_length
-        self_attention_mask = self.bias[None, key_length - query_length : key_length, :key_length]
+        max_length = past_length if past_length > 0 else key_length
+        self_attention_mask = self.bias[None, past_length:key_length, :max_length]
 
         if getattr(self.config, "_flash_attn_2_enabled", False):
             # 2d mask is passed through the layers
