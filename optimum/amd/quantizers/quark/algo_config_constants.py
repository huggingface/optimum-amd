# Copyright 2023 The HuggingFace Team. All rights reserved.
# Licensed under the MIT License.


ALGO_CONFIG_PARAMS = {
    "llama": {
        "scaling_layers": [
            {
                "prev_op": "input_layernorm",
                "layers": ["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj"],
                "inp": "self_attn.q_proj",
                "module2inspect": "self_attn",
                "has_kwargs": True,
                "help": "attention input",
            },
            {
                "prev_op": "self_attn.v_proj",
                "layers": ["self_attn.o_proj"],
                "inp": "self_attn.o_proj",
                "module2inspect": None,
                "has_kwargs": False,
                "help": "attention out, Please refer to https://github.com/mit-han-lab/llm-awq/pull/67#issue-1850622696, if module.self_attn.v_proj.weight.shape == module.self_attn.o_proj.weight.shape",
                "condition": "module.self_attn.v_proj.weight.shape == module.self_attn.o_proj.weight.shape",
            },
            {
                "prev_op": "post_attention_layernorm",
                "layers": ["mlp.gate_proj", "mlp.up_proj"],
                "inp": "mlp.gate_proj",
                "module2inspect": "mlp",
                "has_kwargs": False,
                "help": "linear 1",
            },
            {
                "prev_op": "mlp.up_proj",
                "layers": ["mlp.down_proj"],
                "inp": "mlp.down_proj",
                "module2inspect": None,
                "has_kwargs": False,
                "help": "linear 2",
            },
        ],
        "inside_layer_modules": [
            "self_attn.k_proj",
            "self_attn.v_proj",
            "self_attn.q_proj",
            "self_attn.o_proj",
            "mlp.up_proj",
            "mlp.gate_proj",
            "mlp.down_proj",
        ],
        "model_decoder_layers": "model.layers",
        "embedding_layers": ["model.embed_tokens"],
    },
    "mistral": {
        "scaling_layers": [
            {
                "prev_op": "input_layernorm",
                "layers": ["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj"],
                "inp": "self_attn.q_proj",
                "module2inspect": "self_attn",
                "has_kwargs": True,
                "help": "attention input",
            },
            {
                "prev_op": "self_attn.v_proj",
                "layers": ["self_attn.o_proj"],
                "inp": "self_attn.o_proj",
                "module2inspect": None,
                "has_kwargs": False,
                "help": "attention out, Please refer to https://github.com/mit-han-lab/llm-awq/pull/67#issue-1850622696, if module.self_attn.v_proj.weight.shape == module.self_attn.o_proj.weight.shape",
                "condition": "module.self_attn.v_proj.weight.shape == module.self_attn.o_proj.weight.shape",
            },
            {
                "prev_op": "post_attention_layernorm",
                "layers": ["mlp.gate_proj", "mlp.up_proj"],
                "inp": "mlp.gate_proj",
                "module2inspect": "mlp",
                "has_kwargs": False,
                "help": "linear 1",
            },
            {
                "prev_op": "mlp.up_proj",
                "layers": ["mlp.down_proj"],
                "inp": "mlp.down_proj",
                "module2inspect": None,
                "has_kwargs": False,
                "help": "linear 2",
            },
        ],
        "inside_layer_modules": [
            "self_attn.k_proj",
            "self_attn.v_proj",
            "self_attn.q_proj",
            "self_attn.o_proj",
            "mlp.up_proj",
            "mlp.gate_proj",
            "mlp.down_proj",
        ],
        "model_decoder_layers": "model.layers",
        "embedding_layers": ["model.embed_tokens"],
    },
    "opt": {
        "scaling_layers": [
            {
                "prev_op": "self_attn_layer_norm",
                "layers": ["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj"],
                "inp": "self_attn.q_proj",
                "module2inspect": "self_attn",
                "has_kwargs": True,
                "help": "attention input",
            },
            {
                "prev_op": "self_attn.v_proj",
                "layers": ["self_attn.out_proj"],
                "inp": "self_attn.out_proj",
                "module2inspect": None,
                "has_kwargs": False,
                "help": "attention out",
            },
            {
                "prev_op": "final_layer_norm",
                "layers": ["fc1"],
                "inp": "fc1",
                "module2inspect": None,
                "has_kwargs": False,
                "help": "linear 1",
            },
            {
                "prev_op": "fc1",
                "layers": ["fc2"],
                "inp": "fc2",
                "module2inspect": None,
                "has_kwargs": False,
                "help": "linear 2",
            },
        ],
        "inside_layer_modules": [
            "self_attn.k_proj",
            "self_attn.v_proj",
            "self_attn.q_proj",
            "self_attn.out_proj",
            "fc1",
            "fc2",
        ],
        "model_decoder_layers": "model.decoder.layers",
        "embedding_layers": ["model.decoder.embed_tokens", "model.decoder.embed_positions"],
    },
    "qwen2": {
        "scaling_layers": [
            {
                "prev_op": "input_layernorm",
                "layers": ["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj"],
                "inp": "self_attn.q_proj",
                "module2inspect": "self_attn",
                "has_kwargs": True,
                "help": "attention input",
            },
            {
                "prev_op": "self_attn.v_proj",
                "layers": ["self_attn.o_proj"],
                "inp": "self_attn.o_proj",
                "module2inspect": None,
                "has_kwargs": False,
                "help": "attention out, Please refer to https://github.com/mit-han-lab/llm-awq/pull/67#issue-1850622696, if module.self_attn.v_proj.weight.shape == module.self_attn.o_proj.weight.shape",
            },
            {
                "prev_op": "post_attention_layernorm",
                "layers": ["mlp.gate_proj", "mlp.up_proj"],
                "inp": "mlp.gate_proj",
                "module2inspect": "mlp",
                "has_kwargs": False,
                "help": "linear 1",
            },
            {
                "prev_op": "mlp.up_proj",
                "layers": ["mlp.down_proj"],
                "inp": "mlp.down_proj",
                "module2inspect": None,
                "has_kwargs": False,
                "help": "linear 2",
            },
        ],
        "inside_layer_modules": [
            "self_attn.k_proj",
            "self_attn.v_proj",
            "self_attn.q_proj",
            "self_attn.o_proj",
            "mlp.up_proj",
            "mlp.gate_proj",
            "mlp.down_proj",
        ],
        "model_decoder_layers": "model.layers",
        "embedding_layers": ["model.embed_tokens"],
    },
}
