<!--Copyright 2023 The HuggingFace Team. All rights reserved.
Licensed under the MIT License.
-->

# Using Hugging Face libraries on AMD GPUs

Hugging Face libraries supports natively AMD Instinct MI210, MI250 and MI300 GPUs. For other [ROCm-powered](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus) GPUs, the support has currently not been validated but most features are expected to be used smoothly.

The integration is summarized here.

### Flash Attention 2

Flash Attention 2 is available on ROCm (validated on MI210, MI250 and MI300) through [ROCm/flash-attention](https://github.com/ROCm/flash-attention) library, and can be used in [Transformers](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2):

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-7b")

with torch.device("cuda"):
    model = AutoModelForCausalLM.from_pretrained(
        "tiiuae/falcon-7b",
        torch_dtype=torch.float16,
        use_flash_attention_2=True,
)
```

We recommend using [this example Dockerfile](https://github.com/huggingface/optimum-amd/blob/main/docker/transformers-pytorch-amd-gpu-flash/Dockerfile) to use Flash Attention on ROCm, or to follow the [official installation instructions](https://github.com/ROCm/flash-attention#amd-gpurocm-support).

### GPTQ quantization

[GPTQ](https://arxiv.org/abs/2210.17323) quantized models can be loaded in Transformers, using in the backend [AutoGPTQ library](https://github.com/PanQiWei/AutoGPTQ):

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

tokenizer = AutoTokenizer.from_pretrained("TheBloke/Llama-2-7B-Chat-GPTQ")

with torch.device("cuda"):
    model = AutoModelForCausalLM.from_pretrained(
        "TheBloke/Llama-2-7B-Chat-GPTQ",
        torch_dtype=torch.float16,
    )
```

Hosted wheels are available for ROCm, please check out the [installation instructions](https://github.com/PanQiWei/AutoGPTQ#quick-installation).

### Text Generation Inference library

Hugging Face's [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index) library (TGI) is designed for low latency LLMs serving, and natively supports AMD Instinct MI210, MI250 and MI3O0 GPUs. Please refer to the [Quick Tour section](https://huggingface.co/docs/text-generation-inference/quicktour) for more details.

Using TGI on ROCm with AMD Instinct MI210 or MI250 or MI300 GPUs is as simple as using the docker image [`ghcr.io/huggingface/text-generation-inference:sha-XXXX`](https://huggingface.co/docs/text-generation-inference/quicktour).

Detailed benchmarks of Text Generation Inference on MI300 GPUs will soon be published.

### ONNX Runtime integration

[ðŸ¤— Optimum](https://huggingface.co/docs/optimum/onnxruntime/quickstart) supports running [Transformers](https://github.com/huggingface/transformers) and [Diffusers](https://github.com/huggingface/diffusers) models through [ONNX Runtime](https://onnxruntime.ai/) on ROCm-powered AMD GPUs. It is as simple as:

```python
from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

ort_model = ORTModelForSequenceClassification.from_pretrained(
  "distilbert-base-uncased-finetuned-sst-2-english",
  export=True,
  provider="ROCMExecutionProvider",
)

inp = tokenizer("Both the music and visual were astounding, not to mention the actors performance.", return_tensors="np")
result = ort_model(**inp)
```

Check out more details about the support in [this guide](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu).

### Bitsandbytes quantization

[Bitsandbytes](https://github.com/TimDettmers/bitsandbytes) (integrated in HF's [Transformers](https://huggingface.co/docs/transformers/perf_infer_gpu_one#bitsandbytes) and [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/conceptual/quantization#quantization-with-bitsandbytes)) currently does not officially support ROCm. We are working towards its validation on ROCm and through Hugging Face libraries.

Meanwhile, advanced users may want to use [ROCm/bitsandbytes](https://github.com/ROCm/bitsandbytes/tree/rocm_enabled) fork for now, or a work in progess [community version](https://github.com/TimDettmers/bitsandbytes/pull/756).

### AWQ quantization

[AWQ](https://arxiv.org/abs/2306.00978) quantization, that is supported [in Transformers](https://huggingface.co/docs/transformers/main_classes/quantization#awq-integration) and [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/basic_tutorials/preparing_model#quantization), is currently not available on ROCm GPUs.

We look forward to a port or to the [ongoing developement of a compatible Triton kernel](https://github.com/vllm-project/vllm/blob/qmm/vllm/model_executor/layers/quantized_ops/awq.py).
